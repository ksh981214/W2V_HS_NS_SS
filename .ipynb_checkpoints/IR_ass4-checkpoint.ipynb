{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IR_ass4","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"i22nVR_Ao-RD","colab_type":"code","outputId":"25a96fa6-ba06-456a-d373-38b2406c9eb0","executionInfo":{"status":"ok","timestamp":1573599260746,"user_tz":-540,"elapsed":19733,"user":{"displayName":"­강신환[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"07338171857232659820"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RgsGzojtpMyT","colab_type":"code","outputId":"96f1df88-ed1f-4c8b-977b-f21844d23439","executionInfo":{"status":"ok","timestamp":1573599261712,"user_tz":-540,"elapsed":411,"user":{"displayName":"­강신환[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"07338171857232659820"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd \"drive/My Drive/IR/ass4\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/IR/ass4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L8dIsJZ1nCr8","colab_type":"code","colab":{}},"source":["import heapq\n","import os\n","\n","class HeapNode:\n","    def __init__(self, char, freq):\n","        self.char = char\n","        self.freq = freq\n","        self.left = None\n","        self.right = None\n","\n","    def __lt__(self, other):\n","        if(other == None):\n","            return -1\n","        if(not isinstance(other, HeapNode)):\n","            return -1\n","        return self.freq < other.freq\n","\n","\n","class HuffmanCoding:\n","    def __init__(self):\n","        self.heap = []\n","        self.codes = {}\n","        self.reverse_mapping = {}\n","        self.nonleaf_ind={}\n","\n","    def make_heap(self, frequency):\n","        for key in frequency:\n","            node = HeapNode(key, frequency[key])\n","            heapq.heappush(self.heap, node)\n","\n","    def merge_nodes(self):\n","        while(len(self.heap)>1):\n","            node1 = heapq.heappop(self.heap)\n","            node2 = heapq.heappop(self.heap)\n","\n","            merged = HeapNode(None, node1.freq + node2.freq)\n","            merged.left = node1\n","            merged.right = node2\n","\n","            heapq.heappush(self.heap, merged)\n","\n","\n","    def make_codes_helper(self, root, current_code):\n","        if(root == None):\n","            return\n","\n","        if(root.char != None):\n","            self.codes[root.char] = current_code\n","            self.reverse_mapping[current_code] = root.char\n","            return\n","        self.nonleaf_ind[current_code]={self.num}\n","        self.num+=1\n","        self.make_codes_helper(root.left, current_code + \"0\")\n","        self.make_codes_helper(root.right, current_code + \"1\")\n","\n","\n","    def make_codes(self):\n","        root = heapq.heappop(self.heap)\n","        self.num=0\n","        current_code = \"\"\n","        self.make_codes_helper(root, current_code)\n","\n","\n","    def build(self, frequency):\n","        self.make_heap(frequency)\n","        self.merge_nodes()\n","        self.make_codes()\n","\n","        return self.codes, self.nonleaf_ind"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQcJxNPzkGB_","colab_type":"code","colab":{}},"source":["import torch\n","from random import shuffle\n","from collections import Counter\n","import argparse\n","import random\n","import math\n","import operator\n","import numpy as np\n","\n","import time\n","\n","#import HuffmanCoding\n","\n","def getRandomContext(corpus, C=5):\n","    wordID = random.randint(0, len(corpus) - 1)\n","    \n","    context = corpus[max(0, wordID - C):wordID]\n","    if wordID+1 < len(corpus):\n","        context += corpus[wordID+1:min(len(corpus), wordID + C + 1)]\n","\n","    centerword = corpus[wordID]\n","    context = [w for w in context if w != centerword]\n","    if len(context) > 0:\n","        return centerword, context\n","    else:\n","        return getRandomContext(corpus, C)\n","    \n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","    \n","def get_prob_word_code(word_code, score_vector):\n","    p_word = 1.0\n","    zero_lst=[]\n","    for i, code in enumerate(word_code):\n","        if code == '0':\n","            p_word *= sigmoid(score_vector[i])\n","            zero_lst.append(i)\n","        elif code == '1':\n","            p_word *= (1 - sigmoid(score_vector[i]))\n","        else:\n","            print(\"what is it?\", code)\n","    return p_word, zero_lst\n","\n","def get_activated_node(len_corpus, sampling_num, prob_table, correct_idx):\n","    activated_node_lst = [correct_idx]\n","    lotto_num = random.randint(0, len_corpus - 1)\n","    for i in range(sampling_num):\n","        while correct_idx == lotto_num :    \n","            lotto_num = random.randint(0, len_corpus - 1)\n","        activated_node_lst.append(int(prob_table[lotto_num]))\n","        lotto_num = random.randint(0, len_corpus - 1)\n","    return activated_node_lst\n","\n","def Skipgram(centerWord, contextWord, inputMatrix, outputMatrix, update_system, feed_dict=None):    \n","    if update_system == \"HS\":\n","        #print(\"In SG Func\")\n","        context_word_code = feed_dict['context_word_code']\n","        \n","        #get hidden layer\n","        center_word_vector = inputMatrix[centerWord,:].view(1,-1) #1,D\n","        score_vector = torch.matmul(center_word_vector, torch.t(outputMatrix)) # (1,D) * (D,K) = (1,K)\n","        score_vector = torch.t(score_vector) # (K,1)\n","        \n","        p_context_word,zero_lst = get_prob_word_code(context_word_code, score_vector)\n","        #print(\"In SG Func, Get Prob\")\n","        loss = -torch.log(p_context_word)\n","        \n","        score_grad = score_vector\n","        score_grad[zero_lst] -= 1\n","        \n","        grad_out = torch.matmul(score_grad, center_word_vector) #(K,1) * (1,D) = (K,D)\n","        grad_emb = torch.matmul(torch.t(score_grad), outputMatrix) #(1,K) * (K,D) = (1,D)\n","        \n","    elif update_system == \"NS\":\n","\n","        activated_node_lst =  feed_dict['activated_node_lst']\n","        \n","        center_word_vector = inputMatrix[centerWord,:].view(1,-1) #1,D\n","        \n","        score_vector = torch.matmul(center_word_vector, torch.t(outputMatrix)) # (1,D) * (D,K) = (1,K)\n","        score_vector = torch.t(score_vector) # (K,1)\n","        \n","        loss = 0.0\n","        \n","        for i, idx in enumerate(activated_node_lst):\n","            if idx == contextWord:\n","                context_idx = i\n","                loss -= np.log(sigmoid(score_vector[i]))\n","            else:\n","                loss -= np.log((1 - sigmoid(score_vector[i])))\n","\n","        #get grad\n","        score_grad = score_vector #(K,1)\n","        score_grad[context_idx] -= 1\n","\n","        grad_out = torch.matmul(score_grad, center_word_vector) #(K,1) * (1,D) = (K,D)\n","        grad_emb = torch.matmul(torch.t(score_grad), outputMatrix) #(1,K) * (K,D) = (1,D)\n","        \n","    elif update_system == \"BS\":\n","        #get hidden layer\n","        center_word_vector = inputMatrix[centerWord,:].view(1,-1) #1,D\n","\n","        #score\n","        score_vector = torch.matmul(center_word_vector, torch.t(outputMatrix)) # (1,D) * (D,V) = (1,V)\n","\n","        e = torch.exp(score_vector) \n","        softmax = e / (torch.sum(e, dim=1, keepdim=True)) #1,V\n","\n","        loss = -torch.log(softmax[:,contextWord])\n","\n","        #get grad\n","        softmax_grad = softmax\n","        softmax_grad[:,contextWord] -= 1.0\n","\n","        grad_out = torch.matmul(torch.t(softmax_grad), center_word_vector) #(V,1) * (1,D) = (V,D)\n","        grad_emb = torch.matmul(softmax_grad, outputMatrix) #(1,V) * (V,D) = (1,D)\n","        \n","    else:\n","        print(\"What is it?\")\n","        exit()\n","    return loss, grad_emb, grad_out\n","\n","def CBOW(centerWord, contextWords, inputMatrix, outputMatrix, update_system, feed_dict=None):\n","    if update_system == \"HS\":\n","        center_word_code = feed_dict['center_word_code']\n","        \n","        sum_of_context_words_vector = torch.sum(inputMatrix[contextWords, :],dim=0,keepdim=True) #1,D\n","\n","        score_vector = torch.matmul(sum_of_context_words_vector, torch.t(outputMatrix)) # (1,D) * (D,K) = (1,K)\n","        score_vector = torch.t(score_vector) # (K,1)\n","\n","        p_center_word, zero_lst = get_prob_word_code(center_word_code, score_vector)\n","        \n","        loss = -torch.log(p_center_word)\n","\n","        score_grad = score_vector\n","        score_grad[zero_lst] -= 1\n","\n","        grad_out = torch.matmul(score_grad, sum_of_context_words_vector) #(K,1) * (1,D) = (K,D)\n","        grad_emb = torch.matmul(torch.t(score_grad), outputMatrix) #(1,K) * (K,D) = (1,D)\n","        grad_emb /= 5\n","        \n","    elif update_system == \"NS\":\n","        activated_node_lst =  feed_dict['activated_node_lst']\n","\n","        sum_of_context_words_vector = torch.sum(inputMatrix[contextWords, :],dim=0,keepdim=True) #1,D\n","        \n","        score_vector = torch.matmul(sum_of_context_words_vector, torch.t(outputMatrix)) # (1,D) * (D,K) = (1,K)\n","        score_vector = torch.t(score_vector) # (K,1)\n","        \n","        loss = 0.0\n","        for i, idx in enumerate(activated_node_lst):\n","            if idx == centerWord:\n","                center_idx = i\n","                loss -= np.log(sigmoid(score_vector[i]))\n","            else:\n","                loss -= np.log((1 - sigmoid(score_vector[i])))\n","\n","        #get grad\n","        score_grad = score_vector #(K,1)\n","        score_grad[center_idx] -= 1\n","\n","        grad_out = torch.matmul(score_grad, sum_of_context_words_vector) #(K,1) * (1,D) = (K,D)\n","        grad_emb = torch.matmul(torch.t(score_grad), outputMatrix) #(1,K) * (K,D) = (1,D)\n","        grad_emb /= 5\n","\n","    elif update_system == \"BS\":\n","\n","        sum_of_context_words_vector = torch.sum(inputMatrix[contextWords, :],dim=0,keepdim=True) #1,D\n","\n","        score_vector = torch.matmul(sum_of_context_words_vector, torch.t(outputMatrix)) # (1,D) * (D,V) = (1,V)\n","\n","        e = torch.exp(score_vector) \n","        softmax = e / (torch.sum(e, dim=1, keepdim=True)) #1,V\n","\n","        loss = -torch.log(softmax[:,centerWord])\n","\n","        #get grad\n","        softmax_grad = softmax\n","        softmax_grad[:,centerWord] -= 1.0\n","\n","        grad_out = torch.matmul(torch.t(softmax_grad), sum_of_context_words_vector) #(1,V) * (1,D) = (V,D)\n","        grad_emb = torch.matmul(softmax_grad, outputMatrix) #(1,V) * (V,D) = (1,D)\n","        grad_emb /= 5\n","    \n","    else:\n","        print(\"What is it?\")\n","        exit()\n","    return loss, grad_emb, grad_out\n","\n","\n","def word2vec_trainer(corpus, word2idx, mode, update_system, sub_sampling, dimension, learning_rate, iteration, feed_dict=None):\n","    feed_dict2 = {}\n","\n","    print(\"size of corpus: %d\" % len(corpus))\n","    #Only once\n","    if sub_sampling is True:\n","        print(\"Start SubSampling...\")\n","        prob_of_ss = feed_dict['prob_of_ss']\n","        destiny = np.random.random(size=len(corpus))\n","\n","        subsampling_word = []\n","        for idx, word in enumerate(corpus):\n","            if destiny[idx] < prob_of_ss[word]:\n","                subsampling_word.append(idx) \n","            else:\n","                pass\n","            \n","        corpus = list(np.delete(corpus,subsampling_word))\n","        print(\"Finish SubSampling...\")\n","        print(\"size of corpus(after): %d\" % len(corpus))\n","        \n","    #like 1 epoch\n","    iteration = len(corpus)\n","    if iteration < 100000:\n","        iteration = 100000\n","        \n","    window_size = 5\n","    if update_system == \"HS\":\n","        \n","        word2code = feed_dict['word2code']\n","        nonleaf_idx = feed_dict['non_leaf_code2idx']\n","        code2idx = feed_dict['code2idx']\n","        \n","        W_emb = torch.randn(len(word2idx), dimension) / (dimension**0.5) \n","        W_out = torch.randn(len(nonleaf_idx), dimension) / (dimension**0.5)\n","        \n","        losses=[]\n","        for i in range(iteration): \n","            #Training word2vec using SGD\n","            centerword, context = getRandomContext(corpus, window_size)\n","            center_word_code = word2code[centerword] \n","            context_words_codes = [word2code[i] for i in context]\n","\n","            node_code=''\n","            center_word_activated_node_code_lst = []\n","            for char in center_word_code:\n","                center_word_activated_node_code_lst.append(node_code)\n","                node_code += char\n","            \n","            center_word_activated_node_idx_lst = [list(nonleaf_idx[center_word_activated_node_code])[0] for center_word_activated_node_code in center_word_activated_node_code_lst]\n","            \n","            node_code=''\n","            context_words_activated_node_code_lst = []\n","            for word_code in context_words_codes:\n","                context_word_activated_node_code_lst = []\n","                node_code=''\n","                for char in word_code:\n","                    context_word_activated_node_code_lst.append(node_code)\n","                    node_code += char\n","                context_words_activated_node_code_lst.append(context_word_activated_node_code_lst)\n","\n","            context_words_activated_node_idx_lst = []\n","            for context_word_activated_node_code_lst in context_words_activated_node_code_lst:\n","                context_words_activated_node_idx_lst.append([list(nonleaf_idx[context_word_activated_node_code])[0] for context_word_activated_node_code in context_word_activated_node_code_lst])\n","\n","            centerInd =  word2idx[centerword]\n","            contextInds = [word2idx[context_word] for context_word in context]\n","\n","            if mode == \"CBOW\":\n","                feed_dict2['center_word_code']= center_word_code\n","                \n","                L, G_emb, G_out = CBOW(centerInd, contextInds, W_emb, W_out[center_word_activated_node_idx_lst], update_system, feed_dict2)\n","                W_emb[contextInds] -= learning_rate*G_emb\n","                W_out[center_word_activated_node_idx_lst] -= learning_rate*G_out\n","                losses.append(L)\n","      \n","            elif mode==\"SG\":\n","                #print(i)\n","                #print(contextInds)\n","                for idx, contextInd in enumerate(contextInds):       \n","                    feed_dict2['context_word_code'] = context_words_codes[idx]\n","                    \n","                    L, G_emb, G_out = Skipgram(centerInd, contextInd, W_emb, W_out[context_words_activated_node_idx_lst[idx]], update_system, feed_dict2)\n","                    W_emb[centerInd] -= learning_rate*G_emb.squeeze()\n","                    W_out[context_words_activated_node_idx_lst[idx]] -= learning_rate*G_out\n","                    losses.append(L)\n","            else:\n","                print(\"Unkwnown mode : \"+mode)\n","                exit()\n","            \n","            if i%10000==0:\n","                avg_loss=sum(losses)/len(losses)\n","                print(\"i: %d, Loss : %f\" %(i, avg_loss))\n","                losses=[]\n","                \n","            \n","    elif update_system == \"NS\":\n","        W_emb = torch.randn(len(word2idx), dimension) / (dimension**0.5) \n","        W_out = torch.randn(len(word2idx), dimension) / (dimension**0.5)\n","        losses=[]\n","        \n","        prob_table = feed_dict['prob_table']\n","        sum_of_pow_freq = feed_dict['sum_of_pow_freq']\n","        \n","        sampling_num = 5\n","\n","        for i in range(iteration):\n","            centerword, context = getRandomContext(corpus, window_size)\n","\n","            centerInd =  word2idx[centerword]\n","            contextInds = [word2idx[i] for i in context]            \n","            \n","            if mode == \"CBOW\":\n","                activated_node_lst = get_activated_node(sum_of_pow_freq, sampling_num, prob_table, centerInd)\n","                activated_node_lst.append(centerInd)\n","                \n","                feed_dict2['activated_node_lst']=activated_node_lst\n","                \n","                L, G_emb, G_out = CBOW(centerInd, contextInds, W_emb, W_out[activated_node_lst], update_system, feed_dict2)\n","                W_emb[contextInds] -= learning_rate*G_emb\n","                W_out[activated_node_lst] -= learning_rate*G_out\n","                losses.append(L)\n","                \n","            elif mode==\"SG\":\n","                #print(i)\n","                for contextInd in contextInds:\n","                    activated_node_lst = get_activated_node(sum_of_pow_freq, sampling_num, prob_table, contextInd)\n","                    activated_node_lst.append(contextInd)\n","                    \n","                    feed_dict2['activated_node_lst'] = activated_node_lst\n","                    \n","                    L, G_emb, G_out = Skipgram(centerInd, contextInd, W_emb, W_out[activated_node_lst], update_system, feed_dict2)\n","                    W_emb[centerInd] -= learning_rate*G_emb.squeeze()\n","                    W_out[activated_node_lst] -= learning_rate*G_out\n","                    losses.append(L)\n","            else:\n","                print(\"Unkwnown mode : \"+mode)\n","                exit()\n","            \n","            if i%10000==0:\n","                avg_loss=sum(losses)/len(losses)\n","                print(\"i: %d, Loss : %f\" %(i, avg_loss))\n","                losses=[]\n","    #기존과 동일\n","    elif update_system == \"BS\":\n","        W_emb = torch.randn(len(word2idx), dimension) / (dimension**0.5) \n","        W_out = torch.randn(len(word2idx), dimension) / (dimension**0.5)  \n","        window_size = 5\n","\n","        losses=[]\n","        for i in range(iteration):\n","            #Training word2vec using SGD\n","            centerword, context = getRandomContext(corpus, window_size)\n","            centerInd =  word2idx[centerword]\n","            contextInds = [word2idx[i] for i in context]\n","\n","            if mode==\"CBOW\":\n","                L, G_emb, G_out = CBOW(centerInd, contextInds, W_emb, W_out, update_system)\n","                W_emb[contextInds] -= learning_rate*G_emb\n","                W_out -= learning_rate*G_out\n","                losses.append(L)\n","\n","            elif mode==\"SG\":\n","                #print(i)\n","                for contextInd in contextInds:\n","                    L, G_emb, G_out = Skipgram(centerInd, contextInd, W_emb, W_out, update_system)\n","                    W_emb[centerInd] -= learning_rate*G_emb.squeeze()\n","                    W_out -= learning_rate*G_out\n","                    losses.append(L)\n","            else:\n","                print(\"Unkwnown mode : \"+mode)\n","                exit()\n","\n","            if i%10000==0:\n","                avg_loss=sum(losses)/len(losses)\n","                print(\"i: %d, Loss : %f\" %(i, avg_loss))\n","                losses=[]\n","    \n","    else:\n","        print(\"What is it?\")\n","        exit()\n","\n","\n","    return W_emb, W_out\n","\n","\n","    \n","def get_size(vector):\n","    size = len(vector)\n","    sum = 0.0\n","    for v in vector:\n","        sum += v**2\n","    result = math.sqrt(sum)\n","    return result\n","    \n","#get vector innerproduct\n","def get_innerproduct(v1, v2):\n","    size = len(v1)\n","    result = 0.0\n","    for a,b in zip(v1,v2):\n","        result += a*b\n","    return result\n","    \n","def cosine_similarity(v1, v2, v1_size = None):\n","    if v1_size is not None:\n","        return get_innerproduct(v1,v2) / (v1_size * get_size(v2))\n","    else:\n","        return get_innerproduct(v1,v2) / (get_size(v1) * get_size(v2))\n","    \n","\n","def find_sim_word(emb, w1, w2, w3,ans, word2idx, idx2word):\n","    idx_w1 = word2idx[w1]\n","    idx_w2 = word2idx[w2]\n","    idx_w3 = word2idx[w3]\n","    \n","    vec_w1 = emb[idx_w1]\n","    vec_w2 = emb[idx_w2]\n","    vec_w3 = emb[idx_w3]\n","\n","    vec = vec_w2.sub_(vec_w1).add_(vec_w3)\n","\n","    wor = w1 +\" \"+w2 +\" \"+w3+\" \"+ans\n","    #vec_predicted_size = get_size(list(vec_predicted))\n","    vec_length = torch.sum((vec*vec))**0.5\n","\n","    length = (emb*emb).sum(1)**0.5\n","    vec_normed = vec.reshape(1, -1)/vec_length\n","    sim = (vec_normed@emb.t())[0]/length\n","    values, indices = sim.squeeze().topk(10)\n","    result = {}\n","    topk = []\n","    for i in indices:\n","        topk.append(idx2word[i.item()])\n","    result[wor] = topk\n","    return result\n","    \n","def sim_test(testword, word2idx, idx2word, matrix):\n","    length = (matrix*matrix).sum(1)**0.5\n","    wi = word2idx[testword]\n","    inputVector = matrix[wi].reshape(1,-1)/length[wi]\n","    sim = (inputVector@matrix.t())[0]/length\n","    values, indices = sim.squeeze().topk(5)\n","    \n","    print()\n","    print(\"===============================================\")\n","    print(\"The most similar words to \\\"\" + testword + \"\\\"\")\n","    for ind, val in zip(indices,values):\n","        print(idx2word[ind.item()]+\":%.3f\"%(val,))\n","    print(\"===============================================\")\n","    print()\n","\n","def test(p, m, u, s):\n","    # parser = argparse.ArgumentParser(description='Word2vec')\n","    # parser.add_argument('part', metavar='partition', type=str,\n","    #                     help='\"part\" if you want to train on a part of corpus, \"full\" if you want to train on full corpus')\n","    # parser.add_argument('mode', metavar='mode', type=str,\n","    #                     help='\"SG\" for skipgram, \"CBOW\" for CBOW')\n","    # parser.add_argument('update_system', metavar='update_system', type=str,\n","    #                     help='\"HS for Hierarchical Softmax, NS for Negative Sampling, BS for Basic Softmax')\n","    # parser.add_argument('sub_sampling', metavar='sub_sampling', type=bool,\n","    #                     help='true for sub_sampling or false for not')\n","    # args = parser.parse_args()\n","    # part = args.part\n","    # mode = args.mode\n","    # update_system = args.update_system\n","    # sub_sampling = args.sub_sampling\n","\n","    part = p\n","    mode = m\n","    update_system = u\n","    sub_sampling = s\n","    \n","    \n","    start = time.time()\n","    print(\"loading...\")\n","\n","    if part==\"part\":\n","        text = open('text8',mode='r').readlines()[0][:1000000] #Load a part of corpus for debugging\n","    elif part==\"full\":\n","        text = open('text8',mode='r').readlines()[0] #Load full corpus for submission\n","    else:\n","        print(\"Unknown argument : \" + part)\n","        exit()\n","        \n","    \n","    print(\"tokenizing...\")\n","    corpus = text.split()\n","    frequency = Counter(corpus) #dict\n","    processed = []\n","    #Discard rare words\n","    for word in corpus:\n","        if frequency[word]>4:\n","            processed.append(word)\n","            \n","    frequency = Counter(processed)\n","    vocabulary = set(processed)\n","    \n","    #Assign an index number to a word\n","    word2idx = {}\n","    word2idx[\" \"]=0\n","    i = 1\n","    for word in vocabulary:\n","        word2idx[word] = i\n","        i+=1\n","    idx2word = {}\n","    for k,v in word2idx.items():\n","        idx2word[v]=k\n","    \n","    feed_dict = {} #update_system에 따라 필요한 것 전달하는 용도\n","    \n","    '''\n","    Sub_Sampling or Not\n","    '''\n","    if sub_sampling == True:\n","        #t = 0.5\n","        t=0.5\n","        prob_of_ss = {}\n","        for word, freq in frequency.items():\n","            prob_of_ss[word] = 1- (np.sqrt(t / freq)) #P(w_i) , discard frequent words with prob\n","\n","        feed_dict['prob_of_ss'] = prob_of_ss  \n","    '''\n","    For Hierarchical Softmax\n","    '''\n","    if update_system == \"HS\":\n","        '''\n","        Huffman Coding\n","        '''\n","        print(\"Start Huffman Coding\")\n","\n","        HC = HuffmanCoding()\n","        _,_ = HC.build(frequency) \n","\n","        print(\"Finish Huffman Coding\")\n","\n","        code2idx = {}\n","        \n","        for word,code in HC.codes.items():\n","            code2idx[code] = word2idx[word]\n","        \n","        word2code = HC.codes\n","        code2word = HC.reverse_mapping\n","        \n","        #word 관련\n","        feed_dict['word2code']= word2code\n","        feed_dict['code2word']= code2word\n","        feed_dict['code2idx']= code2idx\n","        feed_dict['idx2word']= idx2word\n","        \n","        #non_leaf 관련\n","        feed_dict['non_leaf_code2idx'] = HC.nonleaf_ind\n","        \n","    '''\n","    For Negative Sampling\n","    '''\n","    if update_system == \"NS\":        \n","        pow_of_freq = {}\n","        for word, freq in frequency.items():\n","            pow_of_freq[word] = math.pow(freq,0.75)\n","        \n","        sum_of_pow_freq = 0\n","        for freq in pow_of_freq.values():\n","            sum_of_pow_freq += freq\n","        #테이블 방법\n","        prob_table = np.zeros(int(sum_of_pow_freq))\n","        idx = 0\n","        for word, freq in pow_of_freq.items():\n","            freq = int(freq)\n","            prob_table[idx:idx+freq] = word2idx[word]\n","            idx = idx+freq \n","\n","        feed_dict['prob_table'] = prob_table\n","        feed_dict['sum_of_pow_freq']= int(sum_of_pow_freq) \n","        \n","    '''\n","    For Basic Softmax\n","    '''\n","    if update_system == \"BS\":\n","        pass\n","    dim = 300\n","    learning_rate = 0.05\n","    iteration = 100000\n","    emb,_ = word2vec_trainer(processed, word2idx, mode, update_system, sub_sampling, dim, learning_rate, iteration, feed_dict)\n","    \n","    consume_time = time.time() - start\n","    \n","    print(\"mode: \", mode)\n","    print(\"update_system: \",update_system)\n","    print(\"sub_sampling: \",sub_sampling)\n","    print(\"consume_time: \", consume_time)\n","    print(\"dim: \",dim)\n","    print(\"learning_rate: \",learning_rate)\n","    print(\"iteration: \", iteration)\n","    '''\n","    Test for Training\n","    '''\n","    #exit()\n","    \n","    #Print similar words\n","    testwords = [\"one\", \"are\", \"he\", \"have\", \"many\", \"first\", \"all\", \"world\", \"people\", \"after\"]\n","    for tw in testwords:\n","    \tsim_test(tw,word2idx,idx2word,emb)\n","    \n","    #Predict the word using the relation of words\n","    print(\"Predict the word using the relation of words\")\n","    ques = open('questions-words.txt',mode='r').readlines()\n","    ques= ques[1:]\n","    num_ques = len(ques)\n","    cantknow = 0\n","    correct_result = []\n","    wrong_result = []\n","\n","    for i in range(num_ques-1):\n","        temp = ques[i].lower().split()\n","        if len(temp) == 4:\n","            w1 = temp[0]\n","            w2 = temp[1]\n","            w3 = temp[2]\n","            ans = temp[3]\n","            if w1 in word2idx.keys() and w2 in word2idx.keys() and w3 in word2idx.keys():\n","                sim_dict = find_sim_word(emb, w1, w2, w3,ans, word2idx, idx2word)\n","                if ans in sim_dict.values():\n","                    correct_result.append(sim_dict)\n","                else:\n","                    wrong_result.append(sim_dict)\n","            else:\n","                cantknow += 1\n","        else:\n","            cantknow += 1\n","    #print(\"correct: \", correct[:-1])\n","    #print(\"can't know: \", correct[-1])\n","    #print(\"num_questions: {}\".format(num_ques) )\n","    print(\"correct_result: {}\".format(len(correct_result)))\n","    print(correct_result)\n","    print(\"wrong_result: {}\".format(len(wrong_result)))\n","    print(wrong_result)\n","    print(\"cantknow: {}\".format(cantknow))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iOetxVjQnXUC","colab_type":"code","colab":{}},"source":["test(\"full\", \"CBOW\", \"BS\", True) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMPKfQ94tVcs","colab_type":"code","outputId":"9186dd33-b963-49df-ee18-3938c106bfde","executionInfo":{"status":"ok","timestamp":1573570015189,"user_tz":-540,"elapsed":1358162,"user":{"displayName":"­강신환[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"07338171857232659820"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1O2UQPgNXEwVbWwxm1XVjFQg7vON-6-Si"}},"source":["test(\"full\", \"CBOW\", \"HS\", True) "],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"wiZ60vrZy43M","colab_type":"code","outputId":"2c8b2282-9978-4c36-961b-4b86c61283d9","executionInfo":{"status":"ok","timestamp":1573571759883,"user_tz":-540,"elapsed":1222943,"user":{"displayName":"­강신환[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"07338171857232659820"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1sJitqDLkZ32WaSA-_SZ7E3HVzCIexOBM"}},"source":["test(\"full\", \"CBOW\", \"NS\", True) "],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"IBOWpb8THo8_","colab_type":"code","outputId":"69a39686-3dce-48bf-aaee-86d26d900e6c","executionInfo":{"status":"ok","timestamp":1573589213573,"user_tz":-540,"elapsed":15792757,"user":{"displayName":"­강신환[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"07338171857232659820"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fuaIi16uQuxGtZiVkREy9zl-YSwCRQLa"}},"source":["test(\"full\", \"CBOW\", \"HS\", False) "],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"XzrpYfzOykAn","colab_type":"code","outputId":"b49be327-057f-4776-8007-dbc5505f492b","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1NufMbIpPwGFdbQYkAfp_8itn3wqWEX9g"},"executionInfo":{"status":"ok","timestamp":1573604579733,"user_tz":-540,"elapsed":4175172,"user":{"displayName":"­강신환[ 학부재학 / 컴퓨터학과 ]","photoUrl":"","userId":"07338171857232659820"}}},"source":["test(\"full\", \"SG\", \"HS\", True) "],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"N_hfGUH1Hmfa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"80a2b8bb-cc0a-482b-829d-d44d7aa23dcf"},"source":["test(\"full\", \"SG\", \"NS\", True) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["loading...\n","tokenizing...\n","size of corpus: 16718844\n","Start SubSampling...\n","Finish SubSampling...\n","size of corpus(after): 365157\n","i: 0, Loss : 4.866515\n","i: 10000, Loss : 4.849478\n","i: 20000, Loss : 4.846016\n","i: 30000, Loss : 4.844040\n","i: 40000, Loss : 4.840141\n","i: 50000, Loss : 4.839386\n","i: 60000, Loss : 4.837358\n","i: 70000, Loss : 4.835575\n","i: 80000, Loss : 4.833730\n","i: 90000, Loss : 4.834391\n","i: 100000, Loss : 4.833457\n","i: 110000, Loss : 4.834292\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TDiRKNi8QcED","colab_type":"code","colab":{}},"source":["test(\"full\", \"SG\", \"BS\", True) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8LFjFwDTdmE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}